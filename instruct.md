
---

## Обзор Проекта `ai_agent_project`

Этот проект представляет собой гибкую архитектуру для создания AI-агентов, способных взаимодействовать с пользователем, использовать внешние инструменты (например, калькулятор, веб-поиск), получать контекст из векторных баз данных и обрабатывать аудиоввод через транскрибацию.

**Основные возможности:**

*   **Модульная архитектура:** Легкое переключение между различными LLM, инструментами, реализациями памяти и векторных хранилищ благодаря использованию абстрактных интерфейсов.
*   **Обработка текстового и файлового ввода:** Агент может обрабатывать текстовые запросы напрямую, а также анализировать содержимое текстовых файлов.
*   **Аудио-ввод:** Поддержка транскрибации аудиофайлов с использованием OpenAI Whisper API, включая автоматическую конвертацию форматов.
*   **Мониторинг локальной папки:** Автоматическая обработка новых аудиофайлов, появляющихся в заданной директории.
*   **Использование инструментов:** Агент может вызывать внешние инструменты (например, калькулятор, веб-поиск) для выполнения конкретных задач.
*   **Извлечение информации (RAG):** Интеграция с векторными базами данных (ChromaDB, Supabase) для извлечения релевантного контекста и обогащения ответов LLM.
*   **Расширяемость:** Возможность добавления собственных LLM, инструментов, хранилищ памяти, векторных баз данных и пользовательских обработчиков (плагинов) на различных этапах работы агента.

**Архитектура:**

Проект построен на принципах модульности и расширяемости, разделяя обязанности между следующими ключевыми компонентами:

*   **`core/`**: Содержит основную логику агента (`AIAgent`) и менеджер инструментов (`ToolManager`).
*   **`interfaces/`**: Определяет абстрактные базовые классы (интерфейсы) для всех подключаемых компонентов, таких как LLM, инструменты, память и векторные хранилища. Это позволяет легко заменять реализации без изменения основной логики агента.
*   **`implementations/`**: Включает конкретные реализации интерфейсов (например, `OpenAI_LLM`, `CalculatorTool`, `ChromaDBStore`).
*   **`utils/`**: Содержит вспомогательные модули для транскрибации аудио и мониторинга файлов.
*   **`config.py`**: Файл для централизованной настройки различных параметров.
*   **`main.py`**: Главный файл, который инициализирует все компоненты, настраивает агента и запускает основной цикл взаимодействия.

---

## Описание Модулей и Их Использование

Давайте подробно рассмотрим каждый файл и его роль в проекте.

### 1. `config.py`

*   **Назначение:** Централизованный файл для хранения глобальных настроек проекта, таких как параметры транскрибации, путь для мониторинга файлов и интервалы, а также, возможно, общие настройки LLM.
*   **Ключевые переменные:**
    *   `MONITOR_DIRECTORY`: Путь к директории, которую `LocalFileMonitor` будет отслеживать. Если пустая строка, мониторинг отключен.
    *   `MONITOR_INTERVAL_SECONDS`: Интервал (в секундах) между проверками `MONITOR_DIRECTORY` на наличие новых файлов.
    *   `OPENAI_API_KEY`: Указывает, что ключ загружается из `.env`.
    *   `LLM_MODEL_NAME`: Закомментированные примеры названий моделей для LLM.
*   **Как использовать/подключать:** Этот файл импортируется в `main.py` (`import config`), и его переменные используются напрямую для настройки других модулей (например, `LocalFileMonitor`).

### 2. `main.py`

*   **Назначение:** Главный исполнительный файл проекта. Он отвечает за:
    *   Загрузку переменных окружения.
    *   Инициализацию всех необходимых компонентов (LLM, память, векторное хранилище, инструменты, транскрибатор, файловый монитор).
    *   Настройку и запуск `AIAgent`.
    *   Обработку аргументов командной строки (для обработки одного аудиофайла или запуска мониторинга).
    *   Реализацию интерактивного режима чата.
    *   Обработку коллбэков от файлового монитора.
    *   Демонстрацию добавления данных в векторное хранилище.
    *   Демонстрацию использования пользовательских плагинов (middleware).
*   **Ключевые компоненты:**
    *   Инициализация `AudioTranscriber`, `LocalFileMonitor`.
    *   Создание экземпляров LLM (по умолчанию `OpenRouter_LLM`), памяти (`ChatHistoryMemory`), векторного хранилища (`SupabaseVectorStore`).
    *   Регистрация инструментов (`CalculatorTool`, `GoogleCSESearchTool`) в агенте.
    *   Функция `handle_monitored_audio_file`: асинхронный коллбэк для `LocalFileMonitor`, транскрибирующий аудио и передающий текст агенту.
    *   Функции `my_custom_pre_llm_processor` и `my_custom_final_response_processor`: примеры плагинов для расширения логики агента.
    *   `process_single_audio_input`: функция для обработки одного аудиофайла.
    *   `main()`: асинхронная функция, управляющая всем жизненным циклом приложения.
*   **Как использовать/подключать:** Запускается как основной скрипт Python.
    *   `python main.py` - для интерактивного режима.
    *   `python main.py --audio /path/to/your/audio.mp3` - для обработки одного аудиофайла.
    *   Для мониторинга папки, установите `MONITOR_DIRECTORY` в `config.py`.

### 3. `core/agent.py`

*   **Назначение:** Определяет класс `AIAgent` – центральный компонент, управляющий всем поведением агента. Он координирует взаимодействие между LLM, инструментами, памятью и векторным хранилищем, а также реализует конвейер обработки запросов.
*   **Ключевые классы:**
    *   `AgentContext`: Вспомогательный класс для хранения состояния и данных, передаваемых по конвейеру обработки запроса агентом. Включает ввод пользователя, сырой ответ LLM, вызовы инструментов, результаты инструментов, контекст RAG, финальный ответ, сообщения об ошибках и метаданные.
    *   `AIAgent`:
        *   **Инициализация:** `AIAgent(llm: AbstractLLM, memory: Memory, vector_store: Optional[VectorStore] = None, system_prompt: Optional[str] = None, max_tool_iterations: int = 3)`. Требует экземпляры LLM и памяти. Векторное хранилище и системный промпт опциональны. `max_tool_iterations` ограничивает цепочку вызовов инструментов.
        *   **`register_tool(tool: Tool)`:** Регистрирует инструмент, делая его доступным для агента и LLM.
        *   **`register_processor(stage: str, processor: AgentProcessor)`:** Позволяет добавлять пользовательские функции-обработчики (middleware) на различных этапах конвейера обработки агента:
            *   `'pre_llm'`: Перед вызовом LLM.
            *   `'post_llm'`: После получения ответа от LLM, но до обработки вызовов инструментов.
            *   `'post_tool_execution'`: После выполнения всех инструментов.
            *   `'final_response'`: Перед формированием окончательного ответа пользователю.
        *   **`process_message(text_input: Optional[str], file_input: Optional[str]) -> Union[str, Dict[str, Any]]`:** Основной метод для обработки входящих сообщений. Он управляет циклом: добавление в память, RAG-поиск, вызов LLM, парсинг вызовов инструментов, выполнение инструментов, добавление результатов в память, и так далее, до получения окончательного ответа.
*   **Как использовать/подключать:** Создается в `main.py`. Ему передаются инициализированные экземпляры LLM, памяти и векторного хранилища. Методы `register_tool` и `register_processor` используются для расширения его функциональности. Затем вызывается `process_message` для обработки запросов.

### 4. `core/tool_manager.py`

*   **Назначение:** Управляет регистрацией и выполнением инструментов, доступных для `AIAgent`.
*   **Ключевой класс:** `ToolManager`
    *   **Инициализация:** `ToolManager()`.
    *   **`register_tool(tool: Tool)`:** Добавляет экземпляр инструмента в свой внутренний реестр.
    *   **`get_tool(name: str) -> Optional[Tool]`:** Возвращает экземпляр инструмента по его имени.
    *   **`list_tools() -> List[Dict[str, str]]`:** Предоставляет список всех зарегистрированных инструментов с их именами и описаниями, используемый `AIAgent` для форматирования информации об инструментах для LLM.
    *   **`execute_tool(tool_name: str, **kwargs) -> Any`:** Выполняет метод `execute` соответствующего инструмента, передавая ему аргументы.
*   **Как использовать/подключать:** Используется внутри `AIAgent`. Вы напрямую не взаимодействуете с `ToolManager`; вместо этого вы регистрируете инструменты через `AIAgent.register_tool()`.

### 5. `interfaces/`

Этот каталог содержит абстрактные базовые классы (ABC) для различных компонентов, обеспечивая модульность и взаимозаменяемость. Вы не подключаете их напрямую, но создаете свои классы, которые наследуют их.

*   **`llm.py` (`AbstractLLM`)**
    *   **Назначение:** Определяет интерфейс для любой Большой Языковой Модели.
    *   **Абстрактные методы:**
        *   `generate_response(prompt, system_prompt, history, tools, **kwargs)` (асинхронный): Для генерации текстового ответа или вызовов инструментов.
        *   `get_embedding(text)` (синхронный): Для генерации векторного представления текста.
*   **`memory.py` (`Memory`)**
    *   **Назначение:** Определяет интерфейс для систем управления памятью (историей чата).
    *   **Абстрактные методы:** `add_message(role, content)`, `get_history(limit)`, `clear()`.
*   **`tool.py` (`Tool`)**
    *   **Назначение:** Определяет интерфейс для любого инструмента, который может использовать агент.
    *   **Абстрактные свойства:** `name` и `description`.
    *   **Абстрактный метод:** `execute(**kwargs)`.
*   **`vector_store.py` (`VectorStore`)**
    *   **Назначение:** Определяет интерфейс для векторных хранилищ, используемых для RAG.
    *   **Абстрактные методы:** `add_documents(documents, metadatas)`, `similarity_search(query, k)`, `clear()`.

### 6. `implementations/llms/`

Реализации интерфейса `AbstractLLM`.

*   **`simple_inference_llm.py` (`SimpleInferenceLLM`)**
    *   **Назначение:** Простая фиктивная LLM для тестирования или демонстрации. Имитирует ответы и вызовы инструментов без реального взаимодействия с API.
    *   **Как использовать:** `my_llm = SimpleInferenceLLM()`. Полезно для разработки без затрат на API.
*   **`openai_llm.py` (`OpenAI_LLM`)**
    *   **Назначение:** Интеграция с OpenAI Chat Completion API (для генерации ответов и вызовов инструментов) и OpenAI Embeddings API (для эмбеддингов).
    *   **Как использовать:** `my_llm = OpenAI_LLM(model_name="gpt-3.5-turbo")`. Требует `OPENAI_API_KEY` в `.env`. Использует `AsyncOpenAI` для асинхронных вызовов чата и `OpenAI` для синхронных вызовов эмбеддингов.
*   **`openrouter_llm.py` (`OpenRouter_LLM`)**
    *   **Назначение:** Интеграция с OpenRouter API, который предоставляет доступ к множеству различных LLM через единый API, совместимый с OpenAI. Поддерживает ротацию API-ключей.
    *   **Как использовать:** `my_llm = OpenRouter_LLM(model_name="qwen/qwen3-coder:free")`. Требует `OPENROUTER_API_KEYS` (можно перечислить несколько через запятую для ротации) в `.env`. Для генерации эмбеддингов может использовать `OPENAI_API_KEY`, если он предоставлен. Создает новый асинхронный клиент для каждого запроса, чтобы использовать текущий ключ из ротации.

### 7. `implementations/tools/`

Реализации интерфейса `Tool`.

*   **`calculator_tool.py` (`CalculatorTool`)**
    *   **Назначение:** Инструмент для выполнения математических выражений.
    *   **Как использовать:** Создайте экземпляр `CalculatorTool()` и зарегистрируйте его в агенте: `ai_agent.register_tool(CalculatorTool())`. Агент сам передаст LLM информацию об этом инструменте, и LLM сможет его вызвать, когда потребуется вычисление.
    *   **Важно:** Использует `eval()`, что может быть небезопасным при использовании с недоверенным вводом в продакшене.
*   **`web_search_tool.py` (`GoogleCSESearchTool`)**
    *   **Назначение:** Инструмент для поиска информации в интернете с использованием Google Custom Search JSON API.
    *   **Как использовать:** Создайте экземпляр `GoogleCSESearchTool()` и зарегистрируйте его в агенте: `ai_agent.register_tool(GoogleCSESearchTool())`.
    *   **Требования:** Для работы с реальным поиском требуется настройка Google Custom Search Engine (CSE) и получение `GOOGLE_CSE_API_KEY` и `GOOGLE_CSE_ID` (Search Engine ID). Если ключи не настроены, возвращает фиктивные ответы.

### 8. `implementations/memory/`

Реализации интерфейса `Memory`.

*   **`chat_history_memory.py` (`ChatHistoryMemory`)**
    *   **Назначение:** Простая реализация памяти, которая хранит историю чата (сообщения пользователя, ассистента и инструментов) в оперативной памяти в виде списка словарей.
    *   **Как использовать:** `my_memory = ChatHistoryMemory()`. Передайте экземпляр агенту: `AIAgent(..., memory=my_memory, ...)`.

### 9. `implementations/vector_stores/`

Реализации интерфейса `VectorStore`.

*   **`simple_vector_store.py` (`SimpleVectorStore`)**
    *   **Назначение:** Базовая демонстрационная реализация векторного хранилища в оперативной памяти. Использует предоставленную LLM для генерации эмбеддингов и выполняет упрощенный поиск схожести.
    *   **Как использовать:** `my_vector_store = SimpleVectorStore(llm_for_embedding=my_llm)`. Передайте агенту: `AIAgent(..., vector_store=my_vector_store, ...)`. Не подходит для больших объемов данных или продакшена.
*   **`chromadb_store.py` (`ChromaDBStore`)**
    *   **Назначение:** Интеграция с ChromaDB – легковесной векторной базой данных. Поддерживает как in-memory, так и персистентное хранение данных на диске. Использует `CustomLLMEmbeddingFunction` для генерации эмбеддингов с помощью вашей LLM.
    *   **Как использовать:** `my_vector_store = ChromaDBStore(llm_for_embedding=my_llm, persist_directory="./chroma_db_data")`. Если `persist_directory` не указан, данные будут храниться только в оперативной памяти.
    *   **Важно:** Метод `llm_for_embedding.get_embedding()` должен быть синхронным, что учтено в `OpenAI_LLM` и `OpenRouter_LLM`.
*   **`supabase_store.py` (`SupabaseVectorStore`)**
    *   **Назначение:** Интеграция с Supabase для использования PostgreSQL с расширением `pgvector` в качестве векторного хранилища. Идеально для облачных решений.
    *   **Как использовать:** `my_vector_store = SupabaseVectorStore(llm_for_embedding=my_llm)`. Требует `SUPABASE_URL` и `SUPABASE_ANON_KEY` в `.env`.
    *   **Требования к Supabase:** Предполагается, что у вас настроен Supabase-проект с таблицей для документов и функцией `match_documents` (см. документацию `pgvector` и Supabase для примера такой функции).

### 10. `utils/`

Вспомогательные утилиты.

*   **`audio_transcriber.py` (`AudioTranscriber`)**
    *   **Назначение:** Модуль для транскрибации аудиофайлов в текст с помощью OpenAI Whisper API. Автоматически конвертирует аудио в поддерживаемые форматы (используя `pydub` и `ffmpeg`) при необходимости.
    *   **Как использовать:** Инициализируется в `main.py`: `audio_transcriber = AudioTranscriber()`. Затем вызывается метод `audio_transcriber.transcribe_audio(filepath)`.
    *   **Требования:** `OPENAI_API_KEY` в `.env`. Для конвертации аудио необходимо установить `ffmpeg` в вашей системе (он не является Python-пакетом).
*   **`local_file_monitor.py` (`LocalFileMonitor`)**
    *   **Назначение:** Отслеживает новые файлы в указанной локальной директории. При обнаружении нового файла вызывает заданную асинхронную функцию обратного вызова (callback).
    *   **Как использовать:** Инициализируется в `main.py` с путем к директории, интервалом проверки и асинхронной `callback_func` (например, `handle_monitored_audio_file`). Запускается как `asyncio.Task`: `local_monitor.run_as_task()`.
    *   **Особенности:** Позволяет фильтровать по расширениям файлов.

---

## Подробная Инструкция По Проекту

### 1. Подготовка Окружения

1.  **Клонирование репозитория (если вы еще этого не сделали):**
    ```bash
    git clone https://github.com/prorokusa/ai_agent_project.git
    cd ai_agent_project
    ```

2.  **Создание и активация виртуального окружения:**
    Настоятельно рекомендуется использовать виртуальное окружение для управления зависимостями.
    ```bash
    python -m venv venv
    # Для Windows:
    .\venv\Scripts\activate
    # Для macOS/Linux:
    source venv/bin/activate
    ```

3.  **Установка зависимостей Python:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Установка FFmpeg:**
    Модуль `AudioTranscriber` использует библиотеку `pydub` для конвертации аудиоформатов, которая, в свою очередь, требует наличия FFmpeg в вашей системе.
    *   **Windows:** Скачайте сборку FFmpeg с [ffmpeg.org](https://ffmpeg.org/download.html) и добавьте путь к исполняемым файлам (`bin` директория) в системную переменную PATH.
    *   **macOS:**
        ```bash
        brew install ffmpeg
        ```
    *   **Linux (Debian/Ubuntu):**
        ```bash
        sudo apt update
        sudo apt install ffmpeg
        ```
    *   **Linux (Fedora):**
        ```bash
        sudo dnf install ffmpeg
        ```
    Убедитесь, что FFmpeg установлен, выполнив `ffmpeg -version` в терминале.

5.  **Настройка файла `.env`:**
    Создайте файл с именем `.env` в корневой директории проекта (там же, где `main.py`). Этот файл будет использоваться для хранения ваших конфиденциальных API-ключей.

    Пример содержимого `.env`:
    ```
    OPENAI_API_KEY="ВАШ_КЛЮЧ_OPENAI_API"
    OPENROUTER_API_KEYS="ВАШ_КЛЮЧ_OPENROUTER_1,ВАШ_КЛЮЧ_OPENROUTER_2" # Можно указать несколько через запятую для ротации
    GOOGLE_CSE_API_KEY="ВАШ_КЛЮЧ_GOOGLE_CSE_API"
    GOOGLE_CSE_ID="ВАШ_ИДЕНТИФИКАТОР_ПОИСКОВОЙ_СИСТЕМЫ_GOOGLE"
    SUPABASE_URL="ВАШ_URL_SUPABASE"
    SUPABASE_ANON_KEY="ВАШ_ANON_KEY_SUPABASE"
    ```
    *   **`OPENAI_API_KEY`**: Обязателен для `AudioTranscriber`, `OpenAI_LLM` и для генерации эмбеддингов в `OpenRouter_LLM` (если не настроена отдельная служба эмбеддингов).
    *   **`OPENROUTER_API_KEYS`**: Используется `OpenRouter_LLM`. Получить можно на [openrouter.ai](https://openrouter.ai/).
    *   **`GOOGLE_CSE_API_KEY` и `GOOGLE_CSE_ID`**: Необходимы для работы `GoogleCSESearchTool`. Инструкции по получению:
        1.  Перейдите в [Google Cloud Console](https://console.cloud.google.com/).
        2.  Создайте новый проект или выберите существующий.
        3.  Включите "Custom Search API" для вашего проекта.
        4.  Создайте учетные данные -> "API key".
        5.  Перейдите в [Custom Search Engine](https://programmablesearchengine.google.com/controlpanel/all) и создайте новую поисковую систему. Укажите сайты для поиска или выберите "Search the entire web" (для этого требуется платная подписка на Google Cloud Search API, бесплатный лимит в 100 запросов в день обычно достаточен для тестирования через обычный Custom Search API).
        6.  Получите "Search engine ID" (cx) из панели управления вашей поисковой системы.
    *   **`SUPABASE_URL` и `SUPABASE_ANON_KEY`**: Необходимы для `SupabaseVectorStore`. Получить можно из настроек вашего проекта Supabase (API -> Project Settings -> API).
        *   **Настройка `pgvector` в Supabase:** Убедитесь, что в вашем проекте Supabase включено расширение `pgvector` (Database -> Extensions -> `pgvector`).
        *   **Создание таблицы и функции:** Вам нужно будет создать таблицу `documents` (или любую другую, указанную в `table_name` для `SupabaseVectorStore`) и, вероятно, вспомогательную SQL-функцию `match_documents` для поиска по схожести. Пример для Supabase:
            ```sql
            -- Включить расширение pgvector
            CREATE EXTENSION IF NOT EXISTS vector;

            -- Создать таблицу для ваших документов
            CREATE TABLE documents (
                id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
                content TEXT NOT NULL,
                embedding vector(1536), -- Размерность эмбеддинга (для OpenAI ada-002 это 1536)
                metadata JSONB -- Для хранения метаданных
            );

            -- Создать функцию для поиска по схожести
            CREATE OR REPLACE FUNCTION match_documents(
              query_embedding vector(1536),
              match_count INT DEFAULT NULL,
              filter JSONB DEFAULT '{}'
            )
            RETURNS TABLE(
              id UUID,
              content TEXT,
              metadata JSONB,
              similarity FLOAT
            )
            LANGUAGE plpgsql
            AS $$
            #variable_conflict use_column
            BEGIN
              RETURN QUERY
              SELECT
                id,
                content,
                metadata,
                1 - (documents.embedding <=> query_embedding) AS similarity
              FROM documents
              WHERE metadata @> filter
              ORDER BY documents.embedding <=> query_embedding
              LIMIT match_count;
            END;
            $$;
            ```

6.  **Настройка `config.py`:**
    Откройте `config.py` и настройте следующие параметры по желанию:
    ```python
    # Путь к директории для отслеживания новых аудиофайлов.
    # Оставьте пустой строкой "", если вы не хотите использовать автоматический мониторинг папки.
    MONITOR_DIRECTORY = "" # Например: "./monitored_audio_files" или "/home/user/audio_inputs"

    # Интервал проверки папки на новые файлы в секундах.
    MONITOR_INTERVAL_SECONDS = 5
    ```
    Если вы зададите `MONITOR_DIRECTORY`, то при запуске `main.py` без аргументов, начнется мониторинг указанной папки.

### 2. Запуск Проекта

Проект `main.py` поддерживает несколько режимов запуска:

#### А. Интерактивный режим чата (по умолчанию)

В этом режиме вы можете вводить текстовые запросы агенту через консоль.
```bash
python main.py
```
После запуска вы увидите приглашение `Вы: `. Введите свой запрос.

*   **Примеры команд в интерактивном режиме:**
    *   `Привет, как дела?`
    *   `Сколько будет 123 * 456?` (вызовет `CalculatorTool`)
    *   `Найди в интернете, кто изобрел телефон.` (вызовет `GoogleCSESearchTool`)
    *   `Какое мое любимое хобби?` (использует `SupabaseVectorStore` для RAG)
    *   `file:my_document.txt` - отправит содержимое `my_document.txt` агенту для анализа.
    *   `audio:my_voice_memo.mp3` - транскрибирует аудиофайл и передаст текст агенту.
    *   `exit` - завершит диалог.

#### Б. Обработка одного аудиофайла через аргумент командной строки

Вы можете указать один аудиофайл для транскрибации и обработки агентом при запуске скрипта:
```bash
python main.py --audio /путь/к/вашему/аудиофайлу.mp3
```
*   Агент транскрибирует аудиофайл, а затем обрабатывает полученный текст так, как если бы это был обычный текстовый ввод.
*   Поддерживаются форматы, совместимые с OpenAI Whisper API (mp3, mp4, wav, flac и т.д.), а также форматы, которые `pydub` может конвертировать (например, awb).

#### В. Мониторинг папки для новых аудиофайлов

Если вы настроили `MONITOR_DIRECTORY` в `config.py`, при запуске `main.py` без дополнительных аргументов (как в интерактивном режиме), агент сначала попытается запустить фоновый мониторинг указанной директории.

```bash
# Убедитесь, что MONITOR_DIRECTORY в config.py установлен, например:
# MONITOR_DIRECTORY = "./monitored_audio_files"

# Затем запустите:
python main.py
```
*   Агент начнет отслеживать указанную папку. Когда вы поместите новый аудиофайл в эту директорию, `LocalFileMonitor` обнаружит его, `AudioTranscriber` транскрибирует его, и полученный текст будет автоматически передан агенту для обработки.
*   После запуска мониторинга скрипт не завершится, а будет продолжать работать в фоновом режиме, ожидая новых файлов. Чтобы остановить, нажмите `Ctrl+C`.

### 3. Расширение Проекта

Проект разработан с учетом легкости расширения.

#### А. Добавление новых LLM

1.  Создайте новый файл в `implementations/llms/` (например, `my_custom_llm.py`).
2.  Создайте класс, который наследуется от `interfaces.llm.AbstractLLM`.
3.  Реализуйте абстрактные методы `generate_response` (асинхронный) и `get_embedding` (синхронный).
4.  Импортируйте свой новый класс в `main.py` и используйте его при инициализации `AIAgent`.

#### Б. Создание своих инструментов

1.  Создайте новый файл в `implementations/tools/` (например, `my_new_tool.py`).
2.  Создайте класс, который наследуется от `interfaces.tool.Tool`.
3.  В конструкторе `__init__` вызовите `super().__init__(name="my_tool_name", description="Описание инструмента.")`.
4.  Реализуйте метод `execute(**kwargs)`, который будет содержать логику вашего инструмента.
5.  Импортируйте ваш новый класс в `main.py` и зарегистрируйте его в агенте: `ai_agent.register_tool(MyNewTool())`.
    *   **Важно:** Описание и ожидаемые аргументы инструмента (`parameters` в формате OpenAI Tools) автоматически формируются в `AIAgent._format_tools_for_llm()` для `calculator` и `google_cse_search`. Если вы добавите новый инструмент, вам может потребоваться обновить этот метод, чтобы LLM могла правильно его вызывать. В более сложных агентах метаданные инструмента могут генерироваться динамически из докстрингов или аннотаций типов.

#### В. Реализация другого типа памяти или векторного хранилища

1.  Создайте новый файл в `implementations/memory/` или `implementations/vector_stores/`.
2.  Создайте класс, который наследуется от соответствующего интерфейса (`interfaces.memory.Memory` или `interfaces.vector_store.VectorStore`).
3.  Реализуйте все абстрактные методы.
4.  Импортируйте свой класс в `main.py` и передайте его агенту при инициализации.

#### Г. Использование пользовательских плагинов (обработчиков)

Вы можете вставить свою логику на различных этапах работы агента, зарегистрировав асинхронные функции как процессоры.

1.  Напишите асинхронную функцию, которая принимает `agent: AIAgent` и `context: AgentContext` в качестве аргументов.
2.  Используйте `ai_agent.register_processor('этап_обработки', ваша_функция_обработчик)`.
    *   Доступные этапы: `'pre_llm'`, `'post_llm'`, `'post_tool_execution'`, `'final_response'`.
    *   Объект `context` содержит все данные текущего запроса и может быть изменен вашим обработчиком.

    Примеры можно найти в `main.py`: `my_custom_pre_llm_processor` и `my_custom_final_response_processor`.

### 4. Возможные Проблемы и Решения

*   **`ValueError: OPENAI_API_KEY не установлен.` или аналогичные ошибки с API-ключами:**
    *   **Решение:** Убедитесь, что вы создали файл `.env` в корне проекта и правильно указали все необходимые API-ключи. Перезапустите скрипт.
*   **`FileNotFoundError: FFmpeg не найден.` или ошибки декодирования аудио:**
    *   **Решение:** Установите FFmpeg в вашей операционной системе и убедитесь, что он доступен из PATH. Проверьте правильность пути к аудиофайлу.
*   **Ошибки при подключении к Supabase:**
    *   **Решение:** Проверьте `SUPABASE_URL` и `SUPABASE_ANON_KEY` в вашем `.env`. Убедитесь, что расширение `pgvector` включено в вашем проекте Supabase, а таблица `documents` и функция `match_documents` созданы.
*   **LLM не вызывает инструменты или не дает адекватных ответов:**
    *   **Решение:**
        *   Проверьте, правильно ли зарегистрированы инструменты в `main.py`.
        *   Убедитесь, что системный промпт агента достаточно хорошо объясняет LLM, когда и как использовать инструменты.
        *   Попробуйте изменить `model_name` для LLM на более способную (например, `gpt-4o` или другую модель, хорошо поддерживающую tool calling).
*   **Аудиофайлы не обрабатываются монитором папки:**
    *   **Решение:**
        *   Проверьте, правильно ли установлен `MONITOR_DIRECTORY` в `config.py` и существует ли эта папка.
        *   Убедитесь, что расширение аудиофайла находится в списке `allowed_extensions` в `LocalFileMonitor` (в `main.py` при инициализации монитора).
        *   Проверьте логи на предмет ошибок инициализации `LocalFileMonitor` или `AudioTranscriber`.
*   **Превышение лимитов API:**
    *   **Решение:**
        *   Увеличьте количество API-ключей для `OpenRouter_LLM` (если это проблема OpenRouter).
        *   Проверьте лимиты вашего тарифного плана для OpenAI и Google CSE.
        *   Используйте `SimpleInferenceLLM` для локального тестирования, чтобы сократить использование API.

---

Надеюсь, это подробное описание и инструкции помогут вам эффективно работать с проектом!